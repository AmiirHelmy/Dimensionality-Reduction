{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We will preform a feature selection but instead of trusting a single model to tell us which features are important we could have multiple models each cast their vote on whether we should keep a feature or not. We could then combine the votes to make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6068, 99)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_columns\",100)\n",
    "# pd.set_option(\"display.max_rows\",6070)\n",
    "df1=pd.read_csv('ANSUR_II_FEMALE.csv')\n",
    "df2=pd.read_csv(\"ANSUR_II_MALE.csv\")\n",
    "anser_df_before=pd.concat([df1,df2])\n",
    "anser_df_before.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reducing df rows for my computaional power is kinda week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=anser_df_before.Gender\n",
    "r=anser_df_before.drop(\"Gender\",axis=1)\n",
    "r_train,r_test,t_train,t_test=train_test_split(r,t,test_size=0.8,random_state=1,stratify=t)\n",
    "anser_df=r_train.merge(t_train,left_on=r_train.index,right_on=t_train.index)\n",
    "anser_df.drop(\"key_0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the dataset have 99 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387, 99)\n"
     ]
    }
   ],
   "source": [
    "print(anser_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first droping the non-numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1387, 94)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop=['Branch','Component','Gender','BMI_class','Height_class']\n",
    "anser_df.drop(to_drop,axis=1,inplace=True)\n",
    "anser_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## secound we train the models one by one. We'll be predicting BMI in the ANSUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "y=anser_df.BMI\n",
    "X=anser_df.drop([\"BMI\"],axis=1)\n",
    "\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "encoded = lab_enc.fit_transform(y)\n",
    "utils.multiclass.type_of_target(y.astype('int'))\n",
    "utils.multiclass.type_of_target(encoded)\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,encoded,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use LassoCV() we'll get an R squared of 99% and when we create a mask that tells us whether a feature has a coefficient different from 0 we can see that this is the case for 38 out of 93 features. We'll put this lcv_mask to the side for a moment and move on to the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model explains 95.0% of the test set variance\n",
      "41 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train,y_train)\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test,y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_!=0 \n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second model to train is a random forest regressor model. by wrapping a Recursive Feature Selector or RFE, around the model to have it select the same number of features as the LassoCV() regressor did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 43 features.\n",
      "The model can explain 17.0% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "rfe_rf=RFE(RandomForestClassifier(),verbose=1,step=10,n_features_to_select=41)\n",
    "\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The third model to train is a Gradient Boosting Regressor model. by wrapping a Recursive Feature Selector or RFE, around the model to have it select the same number of features as the LassoCV() regressor did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 43 features.\n",
      "The model can explain 97.0% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=41, step=10, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining 3 feature selectors\n",
    "### I'll combine the votes of the 3 models i built, to decide which features are important into a meta mask. I'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 93.8% of the variance in the test set using 10 features.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm=LinearRegression()\n",
    "scaler=StandardScaler()\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you can see the simple linear model preformed very well on the dataset with only 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
